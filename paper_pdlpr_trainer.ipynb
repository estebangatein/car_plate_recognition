{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12008457,"sourceType":"datasetVersion","datasetId":7554659}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports and utils","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.transforms import Resize\nfrom torchvision.transforms.functional import to_pil_image\nfrom typing import List, Tuple\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn import CTCLoss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:23.847400Z","iopub.execute_input":"2025-06-10T16:11:23.847728Z","iopub.status.idle":"2025-06-10T16:11:23.852808Z","shell.execute_reply.started":"2025-06-10T16:11:23.847703Z","shell.execute_reply":"2025-06-10T16:11:23.851901Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# utils for decoding the labels\n\nprovinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\nalphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n             'X', 'Y', 'Z', 'O']\nads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n       'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n\n# decodes the plate from the file name\ndef decode_plate(label_str):\n    indices = list(map(int, label_str.split('_')))\n    province = provinces[indices[0]]\n    alphabet = alphabets[indices[1]]\n    ad = ''\n    for i in range(2, len(indices)):\n        ad += ads[indices[i]]\n\n    return province + alphabet + ad\n\nfull_charset = provinces[:-1] + alphabets[:-1] + ads[:-1]\nchar_to_idx = {char: idx+1 for idx, char in enumerate(full_charset)}  # leave 0 for CTC blank\nidx_to_char = {idx: char for char, idx in char_to_idx.items()}\n\n# encodes plate for the model\ndef encode_plate(text: str) -> List[int]:\n    return [char_to_idx[c] for c in text if c in char_to_idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:23.853984Z","iopub.execute_input":"2025-06-10T16:11:23.854482Z","iopub.status.idle":"2025-06-10T16:11:23.871970Z","shell.execute_reply.started":"2025-06-10T16:11:23.854458Z","shell.execute_reply":"2025-06-10T16:11:23.871238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch dataset\nclass LicensePlateCCPDDataset(Dataset):\n    def __init__(self, image_dir):\n        self.image_dir = image_dir\n        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        filename = self.image_files[idx]\n        path = os.path.join(self.image_dir, filename)\n    \n        # load image\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n        # bbox from filename\n        parts = filename.split('-')\n        bbox_part = parts[2]\n        x1y1, x2y2 = bbox_part.split('_')\n        x1, y1 = map(int, x1y1.split('~'))\n        x2, y2 = map(int, x2y2.split('~'))\n    \n        # crop given the plate bbox\n        h, w = image.shape[:2]\n        x1, x2 = max(0, x1), min(w, x2)\n        y1, y2 = max(0, y1), min(h, y2)\n        cropped = image[y1:y2, x1:x2]\n    \n        # resize as the paper\n        cropped = cv2.resize(cropped, (144, 48))\n        image_tensor = torch.tensor(cropped, dtype=torch.float32).permute(2, 0, 1) / 255.0\n\n        # plate text\n        plate_raw = parts[4]\n        plate_text = decode_plate(plate_raw)\n    \n        return image_tensor, plate_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:23.873115Z","iopub.execute_input":"2025-06-10T16:11:23.873702Z","iopub.status.idle":"2025-06-10T16:11:23.891532Z","shell.execute_reply.started":"2025-06-10T16:11:23.873677Z","shell.execute_reply":"2025-06-10T16:11:23.890977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creates the dataset and dataloader\ndataset = LicensePlateCCPDDataset(\"/kaggle/input/ccpd-weather/ccpd_weather\")\nloader = DataLoader(dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:23.892441Z","iopub.execute_input":"2025-06-10T16:11:23.892699Z","iopub.status.idle":"2025-06-10T16:11:23.917461Z","shell.execute_reply.started":"2025-06-10T16:11:23.892677Z","shell.execute_reply":"2025-06-10T16:11:23.916756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# image downsampling (better than pooling)\nclass Focus(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels * 4, out_channels, 3, 1, 1)\n\n    def forward(self, x):\n        return self.conv(torch.cat([\n            x[..., ::2, ::2],\n            x[..., ::2, 1::2],\n            x[..., 1::2, ::2],\n            x[..., 1::2, 1::2]\n        ], dim=1))\n\n# convolution sequence block\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch, k=3, s=1, p=1):\n        super().__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p)\n        self.bn = nn.BatchNorm2d(out_ch)  \n        self.act = nn.LeakyReLU(0.1, inplace=False)\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n# residual blocks\nclass ResBlock(nn.Module):\n    def __init__(self, ch):\n        super().__init__()\n        self.block = nn.Sequential(\n            ConvBlock(ch, ch),\n            ConvBlock(ch, ch)\n        )\n        self.bn = nn.BatchNorm2d(ch) \n\n    def forward(self, x):\n        out = self.block(x)\n        return self.bn(x + out)\n\n\n# Image Global Feature Extractor block (combines the previous blocks)\nclass IGFE(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.focus = Focus(3, 64)\n        self.down1 = ConvBlock(64, 128, s=2)\n        self.down2 = ConvBlock(128, 256, s=2)\n        self.res = nn.Sequential(\n            ResBlock(256),\n            ResBlock(256),\n            ResBlock(256),\n            ResBlock(256)\n        )\n        self.conv_out = nn.Conv2d(256, 512, 1)\n\n    def forward(self, x):\n        x = self.focus(x)\n        x = self.down1(x)\n        x = self.down2(x)\n        x = self.res(x)\n        x = torch.clamp(x, -10, 10)  # safety clamp\n        x = self.conv_out(x)\n        return x\n\n# transformer encoding from image\nclass TransformerEncoder(nn.Module):\n    def __init__(self, d_model=512, nhead=8, num_layers=3):\n        super().__init__()\n        self.pos_embed = nn.Parameter(torch.randn(1, 108, d_model)) \n        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n        self.dropout = nn.Dropout(p=0.1)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = x.view(B, C, -1).permute(0, 2, 1)  \n        x = self.dropout(x + self.pos_embed)\n        x = self.encoder(x)\n        return x\n\n\n\n# prediction block (decodes the text)\nclass ParallelDecoder(nn.Module):\n    def __init__(self, d_model=512, num_classes=92):\n        super().__init__()\n        self.head = nn.Linear(d_model, num_classes)\n\n    def forward(self, x):\n        return self.head(x)\n\n# full model (Parallel Deep-Learning License Plate Recognition)\nclass PDLPRModel(nn.Module):\n    def __init__(self, num_classes=92):\n        super().__init__()\n        self.igfe = IGFE()\n        self.encoder = TransformerEncoder()\n        self.decoder = ParallelDecoder(num_classes=num_classes)\n\n        self._init_weights()\n\n\n    def _init_weights(self): # needed because of unstable training\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n    \n\n\n    def forward(self, x):\n        x = self.igfe(x)\n        x = self.encoder(x)\n        x = self.decoder(x)\n\n        return x\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:23.918741Z","iopub.execute_input":"2025-06-10T16:11:23.919008Z","iopub.status.idle":"2025-06-10T16:11:23.935291Z","shell.execute_reply.started":"2025-06-10T16:11:23.918986Z","shell.execute_reply":"2025-06-10T16:11:23.934762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model initialization and utils\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = PDLPRModel(num_classes=92).to(device)\n\nctc_loss = nn.CTCLoss(blank=0, zero_infinity=True, reduction='sum') # reduction defined for stability\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9) # as in paper","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:11:23.981947Z","iopub.execute_input":"2025-06-10T16:11:23.982633Z","iopub.status.idle":"2025-06-10T16:11:24.166525Z","shell.execute_reply.started":"2025-06-10T16:11:23.982610Z","shell.execute_reply":"2025-06-10T16:11:24.165945Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# training loop\nfor epoch in range(50):\n    model.train()\n    total_loss = 0\n\n    for imgs, texts in tqdm(loader):\n        imgs = imgs.to(device)\n\n        # encoding targets\n        encoded_targets = [torch.tensor(encode_plate(t), dtype=torch.long) for t in texts]\n        target_lengths = torch.tensor([len(seq) for seq in encoded_targets], dtype=torch.long)\n        targets = torch.cat(encoded_targets)\n\n        # all have same input length (108 from encoder block)\n        input_lengths = torch.full(size=(imgs.size(0),), fill_value=108, dtype=torch.long)\n\n        # forward\n        logits = model(imgs)\n        log_probs = logits.log_softmax(2).permute(1, 0, 2)\n\n        # loss = simple_ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0)\n        # uses CTC from torch\n        loss = ctc_loss(\n            log_probs,\n            targets,\n            input_lengths,\n            target_lengths\n        )\n        optimizer.zero_grad()\n        loss.backward()  \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    scheduler.step()\n\n    print(f\"Epoch [{epoch}/50] - Loss: {total_loss / len(loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T16:12:22.250208Z","iopub.execute_input":"2025-06-10T16:12:22.251021Z","iopub.status.idle":"2025-06-10T16:12:26.041755Z","shell.execute_reply.started":"2025-06-10T16:12:22.250991Z","shell.execute_reply":"2025-06-10T16:12:26.040583Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saves the weights\ntorch.save(model.state_dict(), \"pdlpr_model_weights.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}