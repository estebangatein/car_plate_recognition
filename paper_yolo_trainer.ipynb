{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12008457,"sourceType":"datasetVersion","datasetId":7554659}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports and utils","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5.git\n%cd yolov5\n!pip install -r requirements.txt --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:49.799510Z","iopub.execute_input":"2025-06-10T12:08:49.799846Z","iopub.status.idle":"2025-06-10T12:08:53.177784Z","shell.execute_reply.started":"2025-06-10T12:08:49.799822Z","shell.execute_reply":"2025-06-10T12:08:53.176724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.transforms import Resize\nfrom torchvision.transforms.functional import to_pil_image\nfrom typing import List, Tuple\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torch.nn import CTCLoss\nfrom models.common import DetectMultiBackend\nfrom utils.torch_utils import select_device\nfrom models.yolo import Model\nfrom utils.loss import ComputeLoss\nimport yaml\nimport urllib.request","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:53.179551Z","iopub.execute_input":"2025-06-10T12:08:53.180341Z","iopub.status.idle":"2025-06-10T12:08:54.340034Z","shell.execute_reply.started":"2025-06-10T12:08:53.180312Z","shell.execute_reply":"2025-06-10T12:08:54.339451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# utils for decoding the labels\n\nprovinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\nalphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n             'X', 'Y', 'Z', 'O']\nads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n       'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n\n# decodes the plate from the file name\ndef decode_plate(label_str):\n    indices = list(map(int, label_str.split('_')))\n    province = provinces[indices[0]]\n    alphabet = alphabets[indices[1]]\n    ad = ''\n    for i in range(2, len(indices)):\n        ad += ads[indices[i]]\n\n    return province + alphabet + ad\n\nfull_charset = provinces[:-1] + alphabets[:-1] + ads[:-1]\nchar_to_idx = {char: idx+1 for idx, char in enumerate(full_charset)}  # leave 0 for CTC blank\nidx_to_char = {idx: char for char, idx in char_to_idx.items()}\n\ndef encode_plate(text: str) -> List[int]:\n    return [char_to_idx[c] for c in text if c in char_to_idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:54.340874Z","iopub.execute_input":"2025-06-10T12:08:54.341379Z","iopub.status.idle":"2025-06-10T12:08:54.349152Z","shell.execute_reply.started":"2025-06-10T12:08:54.341360Z","shell.execute_reply":"2025-06-10T12:08:54.348580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# torch dataset\nclass LicensePlateCCPDDataset(Dataset):\n    def __init__(self, image_dir, transform=None, img_size=(640, 640)):\n        self.image_dir = image_dir\n        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n        self.transform = transform\n        self.img_size = img_size  # (H, W)\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        filename = self.image_files[idx]\n        path = os.path.join(self.image_dir, filename)\n\n        # load image\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # needed to scale the bounding box\n        original_h, original_w = image.shape[:2]\n\n        # bounding box from filename\n        parts = filename.split('-')\n        bbox_part = parts[2]\n        x1y1, x2y2 = bbox_part.split('_')\n        x1, y1 = map(int, x1y1.split('~'))\n        x2, y2 = map(int, x2y2.split('~'))\n\n        # resize image to 640x640 using + intensity normalization\n        resized_image = cv2.resize(image, self.img_size[::-1])\n        image_tensor = torch.tensor(resized_image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n\n        # adjust bbox to resized scale\n        scale_x = self.img_size[1] / original_w\n        scale_y = self.img_size[0] / original_h\n\n        x1_resized = x1 * scale_x / self.img_size[1]\n        x2_resized = x2 * scale_x / self.img_size[1]\n        y1_resized = y1 * scale_y / self.img_size[0]\n        y2_resized = y2 * scale_y / self.img_size[0]\n\n        bbox = torch.tensor([x1_resized, y1_resized, x2_resized, y2_resized], dtype=torch.float32)\n\n        # plate text\n        plate_raw = parts[4]\n        plate_text = decode_plate(plate_raw)\n\n        return image_tensor, plate_text, bbox","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:54.350965Z","iopub.execute_input":"2025-06-10T12:08:54.351202Z","iopub.status.idle":"2025-06-10T12:08:54.374764Z","shell.execute_reply.started":"2025-06-10T12:08:54.351186Z","shell.execute_reply":"2025-06-10T12:08:54.374214Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creates the dataset and dataloader\ndataset = LicensePlateCCPDDataset(\"/kaggle/input/ccpd-weather/ccpd_weather\")\nloader = DataLoader(dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:54.375324Z","iopub.execute_input":"2025-06-10T12:08:54.375508Z","iopub.status.idle":"2025-06-10T12:08:54.404087Z","shell.execute_reply.started":"2025-06-10T12:08:54.375493Z","shell.execute_reply":"2025-06-10T12:08:54.403557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Detection (YOLO)","metadata":{}},{"cell_type":"code","source":"# utils\nnum_classes = 1  # just detecting plates (needed as YOLO is also a classifier)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Model(cfg='models/yolov5s.yaml', ch=3, nc=num_classes).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:54.404735Z","iopub.execute_input":"2025-06-10T12:08:54.404934Z","iopub.status.idle":"2025-06-10T12:08:54.904003Z","shell.execute_reply.started":"2025-06-10T12:08:54.404920Z","shell.execute_reply":"2025-06-10T12:08:54.903457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# downloads pretrained weights\nurl = 'https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt'\noutput_path = 'yolov5s.pt'\n\nurllib.request.urlretrieve(url, output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:54.904658Z","iopub.execute_input":"2025-06-10T12:08:54.904867Z","iopub.status.idle":"2025-06-10T12:08:55.348088Z","shell.execute_reply.started":"2025-06-10T12:08:54.904849Z","shell.execute_reply":"2025-06-10T12:08:55.347172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loads the weights to the model structure\nweights = torch.load('yolov5s.pt', map_location=device)['model'].float().state_dict()\nmodel_dict = model.state_dict()\n\n# skip last layer (avoid classifier)\nfiltered_weights = {k: v for k, v in weights.items() if not k.startswith('model.24.')}\n\n# loads updated weights\nmodel_dict.update(filtered_weights)\nmodel.load_state_dict(model_dict, strict=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:55.348934Z","iopub.execute_input":"2025-06-10T12:08:55.349150Z","iopub.status.idle":"2025-06-10T12:08:55.458891Z","shell.execute_reply.started":"2025-06-10T12:08:55.349132Z","shell.execute_reply":"2025-06-10T12:08:55.458304Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bbox conversion function to YOLO format\ndef convert_to_yolo_format_normalized(boxes, image_index, default_label=0):\n    \n    if boxes.ndim == 1:\n        boxes = boxes.unsqueeze(0) \n\n    if boxes.numel() == 0:\n        return torch.empty((0, 6))\n    \n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    x_center = (x1 + x2) / 2\n    y_center = (y1 + y2) / 2\n    width = x2 - x1\n    height = y2 - y1\n\n    image_idx = torch.full((boxes.size(0),), fill_value=image_index, dtype=torch.float32)\n    labels = torch.full((boxes.size(0),), fill_value=default_label, dtype=torch.float32)\n\n    # same format as YOLO predictions\n    target = torch.stack([image_idx, labels, x_center, y_center, width, height], dim=1)\n    return target\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:08:55.459685Z","iopub.execute_input":"2025-06-10T12:08:55.459992Z","iopub.status.idle":"2025-06-10T12:08:55.466070Z","shell.execute_reply.started":"2025-06-10T12:08:55.459966Z","shell.execute_reply":"2025-06-10T12:08:55.465329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loads the YOLO loss function (based on IoU)\nwith open('data/hyps/hyp.scratch-low.yaml') as f:\n    hyp = yaml.safe_load(f)\n\nmodel.hyp = hyp\ncompute_loss = ComputeLoss(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:17:46.088658Z","iopub.execute_input":"2025-06-10T12:17:46.089444Z","iopub.status.idle":"2025-06-10T12:17:46.097233Z","shell.execute_reply.started":"2025-06-10T12:17:46.089416Z","shell.execute_reply":"2025-06-10T12:17:46.096484Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# utils\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nmodel = model.to(device)\n\n# training loop\nfor epoch in range(15):\n    model.train()\n    running_loss = 0\n\n    for i, (imgs, _, annotations) in enumerate(tqdm(loader)):\n        imgs = imgs.to(device)\n\n        # need to convert the labels with the function\n        targets = []\n        for j in range(len(imgs)):\n            boxes = annotations[j]\n            yolo_target = convert_to_yolo_format_normalized(boxes, image_index=j)\n            targets.append(yolo_target)\n\n        if len(targets) == 0: # just in case\n            continue\n\n        targets = torch.cat(targets, dim=0).to(device)\n\n        outputs = model(imgs)  # only returning outputs, but still modifying weights (through loss)\n        loss, _ = compute_loss(outputs, targets) # using personalized loss\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch} - Loss: {running_loss / len(loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:19:56.917768Z","iopub.execute_input":"2025-06-10T12:19:56.918047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saves the model\ntorch.save(model.state_dict(), \"my_yolov5.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# check uploading weights works\nmodel = Model('models/yolov5s.yaml', ch=3, nc=1).to(device)\n\nstate_dict = torch.load(\"my_yolov5.pth\", map_location=device)\nmodel.load_state_dict(state_dict) \nmodel.eval() ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}