{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "774180a2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4b00ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: pip install --upgrade pip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pip', 'install', '-r', 'yolov5/requirements.txt', '--quiet'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.run([\"git\", \"clone\", \"https://github.com/ultralytics/yolov5.git\"], check=True)\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"yolov5/requirements.txt\", \"--quiet\"], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124500e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import urllib\n",
    "import torch.nn as nn\n",
    "from utils_esteban import * # utils file for custom datasets and models\n",
    "from torchvision.ops import box_iou, nms\n",
    "import torch.nn.functional as F\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3a05d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"yolov5\")\n",
    "\n",
    "from models.yolo import Model\n",
    "from utils.loss import ComputeLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a499525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c76255",
   "metadata": {},
   "source": [
    "## YOLO model from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4faf712c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads the trained weights\n",
    "model = Model('yolov5/models/yolov5s.yaml', ch=3, nc=1).to(device)\n",
    "\n",
    "state_dict = torch.load(\"model_weights/paper_based/my_yolov5.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eac8c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5a692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the dataset and dataloader\n",
    "# ATTENTION: the dataset is not the same as train, the evaluation is over an OOD dataset\n",
    "dataset = Data_Yolo(\"../../CCPD2019/ccpd_base\")\n",
    "limited_dataset = torch.utils.data.Subset(dataset, indices=range(1000))\n",
    "loader = DataLoader(limited_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7be9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_xyxy(boxes):\n",
    "    \"\"\"Convert [x_center, y_center, w, h] → [x1, y1, x2, y2]\"\"\"\n",
    "    x_c, y_c, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
    "    x1 = x_c - w / 2\n",
    "    y1 = y_c - h / 2\n",
    "    x2 = x_c + w / 2\n",
    "    y2 = y_c + h / 2\n",
    "    return torch.stack([x1, y1, x2, y2], dim=1)\n",
    "\n",
    "def evaluate_yolo(model, dataloader, device, conf_thres=0.25, iou_thres=0.45, iou_eval_thres=0.7, img_size=640):\n",
    "    model.eval()\n",
    "    correct_detections = 0\n",
    "    total_images = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, gt_bboxes in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            preds_raw = model(images)[0]\n",
    "\n",
    "            for i, pred in enumerate(preds_raw):\n",
    "                # filter by confidence\n",
    "                pred = pred[pred[:, 4] >= conf_thres]\n",
    "                if pred.size(0) == 0:\n",
    "                    total_images += 1\n",
    "                    continue\n",
    "\n",
    "                # non-maximum suppression (as in paper)\n",
    "                boxes_xywh = pred[:, :4]\n",
    "                scores = pred[:, 4]\n",
    "                boxes_xyxy = xywh_to_xyxy(boxes_xywh)\n",
    "                keep = nms(boxes_xyxy, scores, iou_thres)\n",
    "                pred_boxes = boxes_xyxy[keep].cpu()\n",
    "\n",
    "                # resizing to ground truth bbox\n",
    "                gt_box_norm = gt_bboxes[i]\n",
    "                x1 = gt_box_norm[0].item() * img_size\n",
    "                y1 = gt_box_norm[1].item() * img_size\n",
    "                x2 = gt_box_norm[2].item() * img_size\n",
    "                y2 = gt_box_norm[3].item() * img_size\n",
    "                gt_box_abs = torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)\n",
    "\n",
    "                # IoU\n",
    "                iou_matrix = box_iou(pred_boxes, gt_box_abs)\n",
    "                max_iou = iou_matrix.max().item()\n",
    "\n",
    "                if max_iou >= iou_eval_thres:\n",
    "                    correct_detections += 1\n",
    "\n",
    "                total_images += 1\n",
    "\n",
    "\n",
    "    accuracy = 100.0 * correct_detections / total_images\n",
    "    print(f\"YOLO Detection Accuracy (IoU > {iou_eval_thres}): {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee2f9d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [01:40<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO Detection Accuracy (IoU > 0.7): 93.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_yolo(model, loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308a8abf",
   "metadata": {},
   "source": [
    "## PDLPR from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a67a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pdlpr = PDLPRModel()\n",
    "model_pdlpr.load_state_dict(torch.load(\"model_weights/paper_based/pdlpr_model_weights.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c07eed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for decoding the labels\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
    "             'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
    "       'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "full_charset = provinces[:-1] + alphabets[:-1] + ads[:-1]\n",
    "char_to_idx = {char: idx+1 for idx, char in enumerate(full_charset)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "# decodes plate from the model\n",
    "def decode_plate_model(indices):\n",
    "    return ''.join([idx_to_char.get(idx, '') for idx in indices if idx != 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be419340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_yolo_and_pdlpr(model_yolo, model_pdlpr, dataloader_yolo, dataloader_pdlpr, device, conf_thres=0.25, iou_thres=0.45, iou_eval_thres=0.7, img_size=640):\n",
    "    model_yolo.eval()\n",
    "    model_pdlpr.eval()\n",
    "\n",
    "    correct_detections = 0\n",
    "    total_images = 0\n",
    "\n",
    "    correct_plates = 0\n",
    "    total_plates = 0\n",
    "    avg_levenshtein = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (images_y, gt_texts, gt_bboxes), (images_p, _, _) in zip(tqdm(dataloader_yolo, desc=\"Evaluating\"), dataloader_pdlpr):\n",
    "            images_y = images_y.to(device)\n",
    "            preds_raw = model_yolo(images_y)[0]\n",
    "\n",
    "            for i, pred in enumerate(preds_raw):\n",
    "                pred = pred[pred[:, 4] >= conf_thres]\n",
    "                if pred.size(0) == 0:\n",
    "                    total_images += 1\n",
    "                    continue\n",
    "                \n",
    "                # non-maximum suppression (as in paper)\n",
    "                boxes_xywh = pred[:, :4]\n",
    "                scores = pred[:, 4]\n",
    "                boxes_xyxy = xywh_to_xyxy(boxes_xywh)\n",
    "                keep = nms(boxes_xyxy, scores, iou_thres)\n",
    "                pred_boxes = boxes_xyxy[keep].cpu()\n",
    "\n",
    "                # ground truth bbox in pixels\n",
    "                gt_box_norm = gt_bboxes[i]\n",
    "                x1 = gt_box_norm[0].item() * img_size\n",
    "                y1 = gt_box_norm[1].item() * img_size\n",
    "                x2 = gt_box_norm[2].item() * img_size\n",
    "                y2 = gt_box_norm[3].item() * img_size\n",
    "                gt_box_abs = torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)\n",
    "\n",
    "                # IoU\n",
    "                iou_matrix = box_iou(pred_boxes, gt_box_abs)\n",
    "                max_iou = iou_matrix.max().item()\n",
    "                if max_iou >= iou_eval_thres:\n",
    "                    correct_detections += 1\n",
    "\n",
    "                    # crop and resize best bbox\n",
    "                    best_idx = torch.argmax(iou_matrix[:, 0]).item()\n",
    "                    pred_box = pred_boxes[best_idx].int()\n",
    "                    x1_, y1_, x2_, y2_ = pred_box.tolist()\n",
    "                    crop = images_p[i, :, y1_:y2_, x1_:x2_] # cropping the other dataset image\n",
    "\n",
    "                    if crop.numel() == 0:\n",
    "                        continue\n",
    "\n",
    "                    crop_resized = F.interpolate(crop.unsqueeze(0), size=(48, 144), mode='bilinear')\n",
    "\n",
    "                    # predict plate text\n",
    "                    output = model_pdlpr(crop_resized.to(device))\n",
    "                    pred_indices = output.argmax(dim=-1).squeeze().tolist()\n",
    "                    pred_text = decode_plate_model(pred_indices)\n",
    "                    gt_text = gt_texts[i]\n",
    "                    total_plates += 1\n",
    "                    if pred_text == gt_text:\n",
    "                        correct_plates += 1\n",
    "                    avg_levenshtein += levenshtein_distance(pred_text, gt_text) / max(len(gt_text), 1)\n",
    "\n",
    "                total_images += 1\n",
    "\n",
    "    detection_accuracy = 100.0 * correct_detections / total_images if total_images > 0 else 0.0\n",
    "    recognition_accuracy = 100.0 * correct_plates / total_plates if total_plates > 0 else 0.0\n",
    "    avg_levenshtein /= total_plates if total_plates > 0 else 1 # normalized\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"YOLO detection accuracy (IoU > {:.2f})\".format(iou_eval_thres): [detection_accuracy],\n",
    "        \"PDLPR recognition accuracy\": [recognition_accuracy],\n",
    "        \"Normalized Levenshtein distance\": [avg_levenshtein]\n",
    "    })\n",
    "    print(metrics_df)\n",
    "    return detection_accuracy, recognition_accuracy, avg_levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18b37bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the dataset and dataloader\n",
    "# ATTENTION: the dataset is not the same as train, the evaluation is over an OOD dataset\n",
    "dataset_yolo = Data_Yolo(\"../../CCPD2019/ccpd_base\")\n",
    "limited_dataset_yolo = torch.utils.data.Subset(dataset_yolo, indices=range(1000))\n",
    "loader_yolo = DataLoader(limited_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# different dataset because of different transformations\n",
    "dataset_pdlpr = Data_Yolo(\"../../CCPD2019/ccpd_base\")\n",
    "limited_dataset_pdlpr = torch.utils.data.Subset(dataset_pdlpr, indices=range(1000))\n",
    "loader_pdlpr = DataLoader(limited_dataset_pdlpr, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f91910cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 125/125 [02:13<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   YOLO detection accuracy (IoU > 0.70)  PDLPR recognition accuracy  \\\n",
      "0                                  93.4                         0.0   \n",
      "\n",
      "   Normalized Levenshtein distance  \n",
      "0                              1.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(93.4, 0.0, 1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_yolo_and_pdlpr(model, model_pdlpr, loader_yolo, loader_pdlpr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b2979",
   "metadata": {},
   "source": [
    "PDLPR model colapsed even with low CTC loss, as the model is known to work, the most probable reason is that the training is not enough (1000 epochs in paper, 300 epochs done here because of limited ressources), or the weights initialization is problematic. \n",
    "\n",
    "93.4% of accuracy obtained on YOLO over an out of distribution dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c164be7",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bb4e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the dataset and dataloader\n",
    "# ATTENTION: the dataset is not the same as train, the evaluation is over an OOD dataset\n",
    "dataset_base = BaselineData(\"../../CCPD2019/ccpd_base\")\n",
    "limited_dataset_base = torch.utils.data.Subset(dataset_base, indices=range(1000))\n",
    "loader_base = DataLoader(limited_dataset_base, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62947f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils for seq2seq\n",
    "\n",
    "# utils for the OCR model\n",
    "CHARS = [\n",
    "    \"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\",\n",
    "    \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\",\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S',\n",
    "    'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'\n",
    "]\n",
    "\n",
    "# needed for the Seq2Seq model\n",
    "SPECIAL = ['<PAD>', '<BOS>', '<EOS>']\n",
    "VOCAB = SPECIAL + sorted(set(CHARS))\n",
    "char2idx = {c: i for i, c in enumerate(VOCAB)}\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "PAD_IDX = char2idx['<PAD>']\n",
    "BOS_IDX = char2idx['<BOS>']\n",
    "EOS_IDX = char2idx['<EOS>']\n",
    "VOCAB_SIZE = len(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "476821e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn = BoundingBoxCNN()\n",
    "model_cnn.load_state_dict(torch.load(\"model_weights/baseline/bounding_boxes_baseline.pth\", map_location=device))\n",
    "\n",
    "model_seq2seq = LicensePlateSeq2Seq(vocab_size=VOCAB_SIZE, max_len=8)\n",
    "model_seq2seq.load_state_dict(torch.load(\"model_weights/baseline/ocr_model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0e6c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline(model_cnn, model_ocr, dataloader, device, iou_eval_thres=0.7):\n",
    "    model_cnn.eval()\n",
    "    model_ocr.eval()\n",
    "\n",
    "    correct_detections = 0\n",
    "    total_images = 0\n",
    "\n",
    "    correct_plates = 0\n",
    "    total_plates = 0\n",
    "    avg_levenshtein = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, gt_texts, gt_bboxes in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            preds = model_cnn(images)\n",
    "            bboxes = preds.squeeze().cpu().numpy()\n",
    "\n",
    "            if bboxes.ndim == 1:\n",
    "                bboxes = bboxes[np.newaxis, :]\n",
    "\n",
    "            H, W = 1160, 720\n",
    "\n",
    "            for i in range(len(bboxes)):\n",
    "                x1, y1, x2, y2 = bboxes[i]\n",
    "                x1, y1, x2, y2 = int(x1 * W), int(y1 * H), int(x2 * W), int(y2 * H)\n",
    "                pred_boxes = torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)\n",
    "\n",
    "                gt_box_norm = gt_bboxes[i]\n",
    "                x1 = gt_box_norm[0].item() * W\n",
    "                y1 = gt_box_norm[1].item() * H\n",
    "                x2 = gt_box_norm[2].item() * W\n",
    "                y2 = gt_box_norm[3].item() * H\n",
    "                gt_box_abs = torch.tensor([[x1, y1, x2, y2]], dtype=torch.float32)\n",
    "\n",
    "                # IoU\n",
    "                iou_matrix = box_iou(pred_boxes, gt_box_abs)\n",
    "                max_iou = iou_matrix.max().item()\n",
    "                if max_iou >= iou_eval_thres:\n",
    "                    correct_detections += 1\n",
    "\n",
    "                    # crop and resize best bbox\n",
    "                    best_idx = torch.argmax(iou_matrix[:, 0]).item()\n",
    "                    pred_box = pred_boxes[best_idx].int()\n",
    "                    x1_, y1_, x2_, y2_ = pred_box.tolist()\n",
    "                    crop = images[i, :, y1_:y2_, x1_:x2_] # cropping the other dataset image\n",
    "\n",
    "                    if crop.numel() == 0:\n",
    "                        continue\n",
    "\n",
    "                    crop_resized = F.interpolate(crop.unsqueeze(0), size=(48, 144), mode='bilinear')\n",
    "\n",
    "                    # predict plate text\n",
    "                    output = model_ocr(crop_resized.to(device), teacher_forcing=False)\n",
    "                    pred_indices_batch = output.argmax(dim=-1).tolist()\n",
    "\n",
    "                    batch_size = len(pred_indices_batch)\n",
    "                    for i in range(batch_size):\n",
    "                        pred_indices = pred_indices_batch[i]\n",
    "                        pred_text = decode_plate_model(pred_indices)\n",
    "                        gt_text = gt_texts[i]\n",
    "\n",
    "                        total_plates += 1\n",
    "\n",
    "                        if pred_text == gt_text:\n",
    "                            correct_plates += 1\n",
    "\n",
    "                        # avoid division by zero\n",
    "                        max_len = max(len(gt_text), 1)\n",
    "                        avg_levenshtein += levenshtein_distance(pred_text, gt_text) / max_len\n",
    "                                \n",
    "\n",
    "                total_images += 1\n",
    "\n",
    "    detection_accuracy = 100.0 * correct_detections / total_images if total_images > 0 else 0.0\n",
    "    recognition_accuracy = 100.0 * correct_plates / total_plates if total_plates > 0 else 0.0\n",
    "    avg_levenshtein /= total_plates if total_plates > 0 else 1 # normalized\n",
    "\n",
    "    metrics_df = pd.DataFrame({\n",
    "        \"CNN detection accuracy (IoU > {:.2f})\".format(iou_eval_thres): [detection_accuracy],\n",
    "        \"Seq2Seq recognition accuracy\": [recognition_accuracy],\n",
    "        \"Normalized Levenshtein distance\": [avg_levenshtein]\n",
    "    })\n",
    "    print(metrics_df)\n",
    "    return detection_accuracy, recognition_accuracy, avg_levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d349a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 125/125 [03:29<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CNN detection accuracy (IoU > 0.70)  Seq2Seq recognition accuracy  \\\n",
      "0                                  5.9                           0.0   \n",
      "\n",
      "   Normalized Levenshtein distance  \n",
      "0                         0.861985  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.9, 0.0, 0.8619854721549628)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_baseline(model_cnn, model_seq2seq, loader_base, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9324c8",
   "metadata": {},
   "source": [
    "Baseline was already observed to be bad in the training, but obtains 5.9% of accuracy and 0 for recognition, but it can be observed that the Levenshtein distance is not 1, in fact, the model predicts the two first characters of the majority of the plates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
