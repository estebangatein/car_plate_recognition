{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T17:59:25.862350Z",
     "iopub.status.busy": "2025-06-09T17:59:25.862066Z",
     "iopub.status.idle": "2025-06-09T17:59:38.314289Z",
     "shell.execute_reply": "2025-06-09T17:59:38.313537Z",
     "shell.execute_reply.started": "2025-06-09T17:59:25.862329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms.functional import to_pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T17:59:57.960841Z",
     "iopub.status.busy": "2025-06-09T17:59:57.960285Z",
     "iopub.status.idle": "2025-06-09T17:59:57.966638Z",
     "shell.execute_reply": "2025-06-09T17:59:57.965956Z",
     "shell.execute_reply.started": "2025-06-09T17:59:57.960817Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# utils for decoding the labels\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
    "             'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
    "       'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "# decodes the plate from the file name\n",
    "def decode_plate(label_str):\n",
    "    indices = list(map(int, label_str.split('_')))\n",
    "    province = provinces[indices[0]]\n",
    "    alphabet = alphabets[indices[1]]\n",
    "    ad = ''\n",
    "    for i in range(2, len(indices)):\n",
    "        ad += ads[indices[i]]\n",
    "\n",
    "    return province + alphabet + ad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T18:00:01.356850Z",
     "iopub.status.busy": "2025-06-09T18:00:01.356592Z",
     "iopub.status.idle": "2025-06-09T18:00:01.363983Z",
     "shell.execute_reply": "2025-06-09T18:00:01.363229Z",
     "shell.execute_reply.started": "2025-06-09T18:00:01.356833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# torch dataset\n",
    "class LicensePlateCCPDDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.image_files[idx]\n",
    "        path = os.path.join(self.image_dir, filename)\n",
    "\n",
    "        # image reading\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "\n",
    "        # bounding box from file name\n",
    "        parts = filename.split('-')\n",
    "        bbox_part = parts[2]\n",
    "        x1y1, x2y2 = bbox_part.split('_')\n",
    "        x1, y1 = map(int, x1y1.split('~'))\n",
    "        x2, y2 = map(int, x2y2.split('~'))\n",
    "\n",
    "        _, img_height, img_width = image.shape\n",
    "        \n",
    "        # normalize the bounding box\n",
    "        x1 = x1 / img_width\n",
    "        x2 = x2 / img_width\n",
    "        y1 = y1 / img_height\n",
    "        y2 = y2 / img_height\n",
    "\n",
    "        bbox = torch.tensor([x1, y1, x2, y2], dtype=torch.float32)\n",
    "\n",
    "        # decodes the plate\n",
    "        plate_raw = parts[4]\n",
    "        plate_text = decode_plate(plate_raw)\n",
    "        \n",
    "        return image, plate_text, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T18:00:04.586451Z",
     "iopub.status.busy": "2025-06-09T18:00:04.585938Z",
     "iopub.status.idle": "2025-06-09T18:00:04.596484Z",
     "shell.execute_reply": "2025-06-09T18:00:04.595916Z",
     "shell.execute_reply.started": "2025-06-09T18:00:04.586422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# creates the dataset and dataloader\n",
    "dataset = LicensePlateCCPDDataset(\"/kaggle/input/ccpd-weather/ccpd_weather\")\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding box model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T18:00:07.281560Z",
     "iopub.status.busy": "2025-06-09T18:00:07.281000Z",
     "iopub.status.idle": "2025-06-09T18:00:07.287183Z",
     "shell.execute_reply": "2025-06-09T18:00:07.286478Z",
     "shell.execute_reply.started": "2025-06-09T18:00:07.281539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CNN model for the bounding box\n",
    "class BoundingBoxCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BoundingBoxCNN, self).__init__()\n",
    "        \n",
    "        # convolutions\n",
    "        self.features = nn.Sequential(\n",
    "        nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "        nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "\n",
    "        # fully connected layers\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 256), nn.ReLU(),\n",
    "            nn.Linear(256, 4),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.regressor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-09T18:00:09.144273Z",
     "iopub.status.busy": "2025-06-09T18:00:09.143729Z",
     "iopub.status.idle": "2025-06-09T18:00:09.439564Z",
     "shell.execute_reply": "2025-06-09T18:00:09.438950Z",
     "shell.execute_reply.started": "2025-06-09T18:00:09.144247Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundingBoxCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (regressor): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=4, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates model + utils for training\n",
    "model = BoundingBoxCNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T18:09:10.064Z",
     "iopub.execute_input": "2025-06-09T18:00:11.289224Z",
     "iopub.status.busy": "2025-06-09T18:00:11.288948Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [08:17<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, MSE Loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 31/313 [00:41<06:16,  1.34s/it]"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, _, bboxes in tqdm(loader):\n",
    "        images, bboxes = images.to(device), bboxes.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, bboxes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, MSE Loss: {running_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# saves the model\n",
    "torch.save(model.state_dict(), \"bounding_boxes_baseline.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# inference on an image\n",
    "\n",
    "# image reading and processing\n",
    "image_bgr = cv2.imread(\"/kaggle/input/ccpd-weather/ccpd_weather/0088-0_1-284~433_435~482-434~481_284~482_285~434_435~433-0_12_22_2_30_28_33-125-27.jpg\")\n",
    "image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "image = image.unsqueeze(0).to(device)\n",
    "\n",
    "# inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "\n",
    "# bounding box processing\n",
    "bbox = output.squeeze().cpu().numpy()\n",
    "\n",
    "x1, y1, x2, y2 = bbox\n",
    "x1, y1, x2, y2 = int(x1*720), int(y1*1160), int(x2*720), int(y2*1160)\n",
    "print(x1, x2, y1, y2)\n",
    "img_copy = image_bgr.copy()\n",
    "\n",
    "# plot of the image with the bounding box\n",
    "cv2.rectangle(img_copy, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\n",
    "img_rgb = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model works, but it is underfitting as the net is not complex enough, result is the mean of all bounding boxes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# utils for the OCR model\n",
    "CHARS = [\n",
    "    \"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\",\n",
    "    \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\",\n",
    "    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S',\n",
    "    'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
    "    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'\n",
    "]\n",
    "\n",
    "# needed for the Seq2Seq model\n",
    "SPECIAL = ['<PAD>', '<BOS>', '<EOS>']\n",
    "VOCAB = SPECIAL + sorted(set(CHARS))\n",
    "char2idx = {c: i for i, c in enumerate(VOCAB)}\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "\n",
    "PAD_IDX = char2idx['<PAD>']\n",
    "BOS_IDX = char2idx['<BOS>']\n",
    "EOS_IDX = char2idx['<EOS>']\n",
    "VOCAB_SIZE = len(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# encodes sequences with the labels for Seq2Seq\n",
    "def encode_sequences(seqs, char2idx, max_len):\n",
    "\n",
    "    bos = char2idx['<BOS>']\n",
    "    eos = char2idx['<EOS>']\n",
    "    pad = char2idx['<PAD>']\n",
    "\n",
    "    encoded = []\n",
    "    for s in seqs:\n",
    "        ids = [bos] + [char2idx.get(c, pad) for c in s] + [eos]\n",
    "        ids = ids[:max_len] \n",
    "        ids += [pad] * (max_len - len(ids)) \n",
    "        encoded.append(ids)\n",
    "\n",
    "    return torch.tensor(encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# OCR model\n",
    "class LicensePlateSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, max_len=10):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # CNNs as encoders\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), \n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2), \n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_enc = nn.Linear(256, hidden_dim)\n",
    "\n",
    "        # LSTM for the text \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img, target_seq=None, teacher_forcing=True):\n",
    "        # encoder\n",
    "        feat = self.encoder(img)              \n",
    "        feat = self.flatten(feat)\n",
    "        # initial states for the LSTM\n",
    "        h0 = torch.tanh(self.fc_enc(feat))    \n",
    "        h0 = h0.unsqueeze(0)                  \n",
    "        c0 = torch.zeros_like(h0)\n",
    "\n",
    "        B = img.size(0)\n",
    "        outputs = []\n",
    "        input_token = torch.full((B, 1), BOS_IDX, dtype=torch.long, device=img.device)  # [B, 1]\n",
    "\n",
    "        # generates the tokens through LSTM\n",
    "        for t in range(self.max_len):\n",
    "            embed = self.embedding(input_token)  \n",
    "            out, (h0, c0) = self.lstm(embed, (h0, c0))\n",
    "            logits = self.output(out.squeeze(1))  \n",
    "            outputs.append(logits.unsqueeze(1))   \n",
    "\n",
    "            # set to true in training to help, uses the true sequence\n",
    "            if teacher_forcing:\n",
    "                input_token = target_seq[:, t].unsqueeze(1)  \n",
    "            else:\n",
    "                input_token = logits.argmax(1).unsqueeze(1) # only for inference\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# initializes the model\n",
    "model = LicensePlateSeq2Seq(vocab_size=VOCAB_SIZE, max_len=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "resize_crop = Resize((48, 144)) # image size considered (biased from the paper)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for img, label_str, bbox in tqdm(loader):\n",
    "        B = img.size(0)\n",
    "        cropped_imgs = []\n",
    "\n",
    "        for i in range(B):\n",
    "            # invert the normalization\n",
    "            x1 = int(bbox[i][0].item() * 720)\n",
    "            y1 = int(bbox[i][1].item() * 1160)\n",
    "            x2 = int(bbox[i][2].item() * 720)\n",
    "            y2 = int(bbox[i][3].item() * 1160)\n",
    "\n",
    "            # crops and resizes the image\n",
    "            crop_tensor = img[i][:, y1:y2, x1:x2]\n",
    "            resized = resize_crop(crop_tensor)\n",
    "            cropped_imgs.append(resized)\n",
    "\n",
    "        # recreates a tensor, translates the labels\n",
    "        images = torch.stack(cropped_imgs).to(device)\n",
    "        target_seq = encode_sequences(label_str, char2idx, max_len=8).to(device)\n",
    "\n",
    "        # inference\n",
    "        output = model(images, target_seq=target_seq, teacher_forcing=True)\n",
    "        loss = criterion(output.view(-1, VOCAB_SIZE), target_seq.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"[Epoch {epoch}] Loss: {total_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# saves the model\n",
    "torch.save(model.state_dict(), \"ocr_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-09T17:58:56.594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# inference for one image\n",
    "model.eval()\n",
    "\n",
    "img, label_str, bbox = next(iter(loader)) \n",
    "img0 = img[1]  # for just one image\n",
    "label = label_str[1]\n",
    "bbox0 = bbox[1]\n",
    "\n",
    "# crops and resize\n",
    "x1 = int(bbox0[0].item() * 720)\n",
    "y1 = int(bbox0[1].item() * 1160)\n",
    "x2 = int(bbox0[2].item() * 720)\n",
    "y2 = int(bbox0[3].item() * 1160)\n",
    "\n",
    "crop_tensor = img0[:, y1:y2, x1:x2]\n",
    "crop_resized = resize_crop(crop_tensor)\n",
    "image = crop_resized.unsqueeze(0).to(device) \n",
    "\n",
    "# inference\n",
    "with torch.no_grad():\n",
    "    output = model(image, teacher_forcing=False) \n",
    "    pred_indices = output.argmax(2).squeeze(0).tolist()\n",
    "\n",
    "# decoding\n",
    "pred_text = ''\n",
    "for idx in pred_indices:\n",
    "    char = idx2char.get(idx, '')\n",
    "    if char == '<EOS>':\n",
    "        break\n",
    "    if char not in ['<PAD>', '<BOS>']:\n",
    "        pred_text += char\n",
    "\n",
    "# plot\n",
    "print(pred_text)\n",
    "pil_img = to_pil_image(crop_tensor)\n",
    "plt.imshow(pil_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model colapses as the data is totally unbalanced. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7554659,
     "sourceId": 12008457,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
