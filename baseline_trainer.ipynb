{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12008457,"sourceType":"datasetVersion","datasetId":7554659}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom tqdm import tqdm\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.transforms import Resize\nfrom torchvision.transforms.functional import to_pil_image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:59:25.862066Z","iopub.execute_input":"2025-06-09T17:59:25.862350Z","iopub.status.idle":"2025-06-09T17:59:38.314289Z","shell.execute_reply.started":"2025-06-09T17:59:25.862329Z","shell.execute_reply":"2025-06-09T17:59:38.313537Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"# utils for decoding the labels\n\nprovinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\nalphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n             'X', 'Y', 'Z', 'O']\nads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n       'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n\n# decodes the plate from the file name\ndef decode_plate(label_str):\n    indices = list(map(int, label_str.split('_')))\n    province = provinces[indices[0]]\n    alphabet = alphabets[indices[1]]\n    ad = ''\n    for i in range(2, len(indices)):\n        ad += ads[indices[i]]\n\n    return province + alphabet + ad\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T17:59:57.960285Z","iopub.execute_input":"2025-06-09T17:59:57.960841Z","iopub.status.idle":"2025-06-09T17:59:57.966638Z","shell.execute_reply.started":"2025-06-09T17:59:57.960817Z","shell.execute_reply":"2025-06-09T17:59:57.965956Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# torch dataset\nclass LicensePlateCCPDDataset(Dataset):\n    def __init__(self, image_dir):\n        self.image_dir = image_dir\n        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        filename = self.image_files[idx]\n        path = os.path.join(self.image_dir, filename)\n\n        # image reading\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n\n        # bounding box from file name\n        parts = filename.split('-')\n        bbox_part = parts[2]\n        x1y1, x2y2 = bbox_part.split('_')\n        x1, y1 = map(int, x1y1.split('~'))\n        x2, y2 = map(int, x2y2.split('~'))\n\n        _, img_height, img_width = image.shape\n        \n        # normalize the bounding box\n        x1 = x1 / img_width\n        x2 = x2 / img_width\n        y1 = y1 / img_height\n        y2 = y2 / img_height\n\n        bbox = torch.tensor([x1, y1, x2, y2], dtype=torch.float32)\n\n        # decodes the plate\n        plate_raw = parts[4]\n        plate_text = decode_plate(plate_raw)\n        \n        return image, plate_text, bbox","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:00:01.356592Z","iopub.execute_input":"2025-06-09T18:00:01.356850Z","iopub.status.idle":"2025-06-09T18:00:01.363983Z","shell.execute_reply.started":"2025-06-09T18:00:01.356833Z","shell.execute_reply":"2025-06-09T18:00:01.363229Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# creates the dataset and dataloader\ndataset = LicensePlateCCPDDataset(\"/kaggle/input/ccpd-weather/ccpd_weather\")\nloader = DataLoader(dataset, batch_size=32, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:00:04.585938Z","iopub.execute_input":"2025-06-09T18:00:04.586451Z","iopub.status.idle":"2025-06-09T18:00:04.596484Z","shell.execute_reply.started":"2025-06-09T18:00:04.586422Z","shell.execute_reply":"2025-06-09T18:00:04.595916Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Bounding box model","metadata":{}},{"cell_type":"code","source":"# CNN model for the bounding box\nclass BoundingBoxCNN(nn.Module):\n    def __init__(self):\n        super(BoundingBoxCNN, self).__init__()\n        \n        # convolutions\n        self.features = nn.Sequential(\n        nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n        nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n        nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n        nn.AdaptiveAvgPool2d((1, 1))\n        )\n\n        # fully connected layers\n        self.regressor = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128, 256), nn.ReLU(),\n            nn.Linear(256, 4),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.regressor(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:00:07.281000Z","iopub.execute_input":"2025-06-09T18:00:07.281560Z","iopub.status.idle":"2025-06-09T18:00:07.287183Z","shell.execute_reply.started":"2025-06-09T18:00:07.281539Z","shell.execute_reply":"2025-06-09T18:00:07.286478Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# creates model + utils for training\nmodel = BoundingBoxCNN()\ncriterion = nn.MSELoss()\noptimizer = Adam(model.parameters(), lr=1e-3)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:00:09.143729Z","iopub.execute_input":"2025-06-09T18:00:09.144273Z","iopub.status.idle":"2025-06-09T18:00:09.439564Z","shell.execute_reply.started":"2025-06-09T18:00:09.144247Z","shell.execute_reply":"2025-06-09T18:00:09.438950Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BoundingBoxCNN(\n  (features): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU()\n    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): AdaptiveAvgPool2d(output_size=(1, 1))\n  )\n  (regressor): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=128, out_features=256, bias=True)\n    (2): ReLU()\n    (3): Linear(in_features=256, out_features=4, bias=True)\n    (4): Sigmoid()\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# training loop\nfor epoch in range(20):\n    model.train()\n    running_loss = 0.0\n    for images, _, bboxes in tqdm(loader):\n        images, bboxes = images.to(device), bboxes.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n\n        loss = criterion(outputs, bboxes)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    print(f\"Epoch {epoch+1}, MSE Loss: {running_loss/len(loader):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T18:00:11.288948Z","iopub.execute_input":"2025-06-09T18:00:11.289224Z","execution_failed":"2025-06-09T18:09:10.064Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 313/313 [08:17<00:00,  1.59s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, MSE Loss: 0.0062\n","output_type":"stream"},{"name":"stderr","text":" 10%|▉         | 31/313 [00:41<06:16,  1.34s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# saves the model\ntorch.save(model.state_dict(), \"bounding_boxes_baseline.pth\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inference on an image\n\n# image reading and processing\nimage_bgr = cv2.imread(\"/kaggle/input/ccpd-weather/ccpd_weather/0088-0_1-284~433_435~482-434~481_284~482_285~434_435~433-0_12_22_2_30_28_33-125-27.jpg\")\nimage = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\nimage = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1) / 255.0\nimage = image.unsqueeze(0).to(device)\n\n# inference\nmodel.eval()\nwith torch.no_grad():\n    output = model(image)\n\n# bounding box processing\nbbox = output.squeeze().cpu().numpy()\n\nx1, y1, x2, y2 = bbox\nx1, y1, x2, y2 = int(x1*720), int(y1*1160), int(x2*720), int(y2*1160)\nprint(x1, x2, y1, y2)\nimg_copy = image_bgr.copy()\n\n# plot of the image with the bounding box\ncv2.rectangle(img_copy, (x1, y1), (x2, y2), color=(0, 255, 0), thickness=2)\nimg_rgb = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(8, 6))\nplt.imshow(img_rgb)\nplt.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.593Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model works, but it is underfitting as the net is not complex enough, result is the mean of all bounding boxes.","metadata":{}},{"cell_type":"markdown","source":"## OCR model","metadata":{}},{"cell_type":"code","source":"# utils for the OCR model\nCHARS = [\n    \"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\",\n    \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\",\n    'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S',\n    'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9'\n]\n\n# needed for the Seq2Seq model\nSPECIAL = ['<PAD>', '<BOS>', '<EOS>']\nVOCAB = SPECIAL + sorted(set(CHARS))\nchar2idx = {c: i for i, c in enumerate(VOCAB)}\nidx2char = {i: c for c, i in char2idx.items()}\n\nPAD_IDX = char2idx['<PAD>']\nBOS_IDX = char2idx['<BOS>']\nEOS_IDX = char2idx['<EOS>']\nVOCAB_SIZE = len(VOCAB)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# encodes sequences with the labels for Seq2Seq\ndef encode_sequences(seqs, char2idx, max_len):\n\n    bos = char2idx['<BOS>']\n    eos = char2idx['<EOS>']\n    pad = char2idx['<PAD>']\n\n    encoded = []\n    for s in seqs:\n        ids = [bos] + [char2idx.get(c, pad) for c in s] + [eos]\n        ids = ids[:max_len]  # Truncar si es necesario\n        ids += [pad] * (max_len - len(ids))  # Padding\n        encoded.append(ids)\n\n    return torch.tensor(encoded, dtype=torch.long)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# OCR model\nclass LicensePlateSeq2Seq(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, max_len=10):\n        super().__init__()\n        self.max_len = max_len\n\n        # CNNs as encoders\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # 24x72\n            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),  # 12x36\n            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1))  # 1x1\n        )\n        self.flatten = nn.Flatten()\n        self.fc_enc = nn.Linear(256, hidden_dim)\n\n        # LSTM for the text \n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=1, batch_first=True)\n        self.output = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, img, target_seq=None, teacher_forcing=True):\n        # encoder\n        feat = self.encoder(img)              \n        feat = self.flatten(feat)\n        # initial states for the LSTM\n        h0 = torch.tanh(self.fc_enc(feat))    \n        h0 = h0.unsqueeze(0)                  \n        c0 = torch.zeros_like(h0)\n\n        B = img.size(0)\n        outputs = []\n        input_token = torch.full((B, 1), BOS_IDX, dtype=torch.long, device=img.device)  # [B, 1]\n\n        # generates the tokens through LSTM\n        for t in range(self.max_len):\n            embed = self.embedding(input_token)  \n            out, (h0, c0) = self.lstm(embed, (h0, c0))\n            logits = self.output(out.squeeze(1))  \n            outputs.append(logits.unsqueeze(1))   \n\n            # set to true in training to help, uses the true sequence\n            if teacher_forcing:\n                input_token = target_seq[:, t].unsqueeze(1)  \n            else:\n                input_token = logits.argmax(1).unsqueeze(1) # only for inference\n\n        return torch.cat(outputs, dim=1)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# initializes the model\nmodel = LicensePlateSeq2Seq(vocab_size=VOCAB_SIZE, max_len=8).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"resize_crop = Resize((48, 144)) # image size considered (biased from the paper)\n\n# training loop\nfor epoch in range(20):\n    model.train()\n    total_loss = 0\n\n    for img, label_str, bbox in tqdm(loader):\n        B = img.size(0)\n        cropped_imgs = []\n\n        for i in range(B):\n            # invert the normalization\n            x1 = int(bbox[i][0].item() * 720)\n            y1 = int(bbox[i][1].item() * 1160)\n            x2 = int(bbox[i][2].item() * 720)\n            y2 = int(bbox[i][3].item() * 1160)\n\n            # crops and resizes the image\n            crop_tensor = img[i][:, y1:y2, x1:x2]\n            resized = resize_crop(crop_tensor)\n            cropped_imgs.append(resized)\n\n        # recreates a tensor, translates the labels\n        images = torch.stack(cropped_imgs).to(device)\n        target_seq = encode_sequences(label_str, char2idx, max_len=8).to(device)\n\n        # inference\n        output = model(images, target_seq=target_seq, teacher_forcing=True)  # [B, 10, C]\n        loss = criterion(output.view(-1, VOCAB_SIZE), target_seq.view(-1))   # [B*10, C] vs [B*10]\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f\"[Epoch {epoch}] Loss: {total_loss / len(loader):.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# saves the model\ntorch.save(model.state_dict(), \"ocr_model.pth\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inference for one image\nmodel.eval()\n\nimg, label_str, bbox = next(iter(loader)) \nimg0 = img[1]  # for just one image\nlabel = label_str[1]\nbbox0 = bbox[1]\n\n# crops and resize\nx1 = int(bbox0[0].item() * 720)\ny1 = int(bbox0[1].item() * 1160)\nx2 = int(bbox0[2].item() * 720)\ny2 = int(bbox0[3].item() * 1160)\n\ncrop_tensor = img0[:, y1:y2, x1:x2]\ncrop_resized = resize_crop(crop_tensor)\nimage = crop_resized.unsqueeze(0).to(device) \n\n# inference\nwith torch.no_grad():\n    output = model(image, teacher_forcing=False) \n    pred_indices = output.argmax(2).squeeze(0).tolist()\n\n# decoding\npred_text = ''\nfor idx in pred_indices:\n    char = idx2char.get(idx, '')\n    if char == '<EOS>':\n        break\n    if char not in ['<PAD>', '<BOS>']:\n        pred_text += char\n\n# plot\nprint(pred_text)\npil_img = to_pil_image(crop_tensor)\nplt.imshow(pil_img)\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-09T17:58:56.594Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model colapses as the data is totally unbalanced. ","metadata":{}}]}